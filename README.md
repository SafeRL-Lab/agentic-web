## Agentic Web


The repository is for *Agentic Web* research, in which we investigate various agentic web studies. If any authors do not want their paper to be listed here, please feel free to contact <gshangd[AT]foxmail.com>. (This repository is under actively development. We appreciate any constructive comments and suggestions)


You are more than welcome to update this list! If you find a paper about Safe RL which is not listed here, please

- fork this repository, add it and merge back;
- or report an issue here;

## Agentic Information Retrieval

-[Agentic information retrieval](https://arxiv.org/pdf/2410.09713?) by Zhang, Weinan, Junwei Liao, Ning Li, Kounianhua Du, and Jianghao Lin. 2025
  
## Safety and Security


- [Securing agentic ai: A comprehensive threat model and mitigation framework for generative ai agents](https://arxiv.org/pdf/2504.19956) by Narajala, Vineeth Sai, and Om Narayan.  2025
- [Open challenges in multi-agent security: Towards secure systems of interacting ai agents](https://arxiv.org/pdf/2505.02077) by de Witt, Christian Schroeder. 2025
- [Model context protocol (mcp): Landscape, security threats, and future research directions](https://arxiv.org/pdf/2503.23278?) by Hou, Xinyi, Yanjie Zhao, Shenao Wang, and Haoyu Wang. 2025
- [Ai agents under threat: A survey of key security challenges and future pathways](https://dl.acm.org/doi/pdf/10.1145/3716628) by Deng, Zehang, Yongjian Guo, Changzhou Han, Wanlun Ma, Junwu Xiong, Sheng Wen, and Yang Xiang. 2025
- [Enterprise-grade security for the model context protocol (mcp): Frameworks and mitigation strategies](https://arxiv.org/pdf/2504.08623?) by Narajala, Vineeth Sai, and Idan Habler. 2025
- [Position: AI Safety Must Embrace an Antifragile Perspective](https://openreview.net/pdf?id=WpePuya3Ki) by Jin, Ming, and Hyunin Lee. 2025
- [Red-teaming llm multi-agent systems via communication attacks](https://arxiv.org/pdf/2502.14847) by He, Pengfei, Yupin Lin, Shen Dong, Han Xu, Yue Xing, and Hui Liu. 2025
- [Skin-in-the-game: Decision making via multi-stakeholder alignment in llms](https://arxiv.org/pdf/2405.12933?) by Sel, Bilgehan, Priya Shanmugasundaram, Mohammad Kachuee, Kun Zhou, Ruoxi Jia, and Ming Jin. 2024
- [Ai safety in generative ai large language models: A survey](https://arxiv.org/pdf/2407.18369?) by Chua, Jaymari, Yun Li, Shiyi Yang, Chen Wang, and Lina Yao. 2024
- [Mart: Improving llm safety with multi-round automatic red-teaming](https://arxiv.org/pdf/2311.07689) by Ge, Suyu, Chunting Zhou, Rui Hou, Madian Khabsa, Yi-Chia Wang, Qifan Wang, Jiawei Han, and Yuning Mao. 2023
- [Red teaming language models with language models](https://arxiv.org/pdf/2202.03286) by Perez, Ethan, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 2022
- [Beyond accuracy: Behavioral testing of NLP models with CheckList](https://arxiv.org/pdf/2005.04118) by Ribeiro, Marco Tulio, Tongshuang Wu, Carlos Guestrin, and Sameer Singh.  2020
- [HateCheck: Functional tests for hate speech detection models](https://arxiv.org/pdf/2012.15606) by Röttger, Paul, Bertram Vidgen, Dong Nguyen, Zeerak Waseem, Helen Margetts, and Janet B. 2020
- [The malicious use of artificial intelligence: Forecasting, prevention, and mitigation](https://arxiv.org/pdf/1802.07228) by Brundage, Miles, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe et al. 2018
- [Concrete problems in AI safety](https://arxiv.org/pdf/1606.06565) by Amodei, Dario, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. 2016
